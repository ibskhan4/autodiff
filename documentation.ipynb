{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Automatic differentiation is essentially an algorithmic approach to take the derivatives of functions, and is capable of evaluating the function and its derivative at specified numerical values. The process centers around the concept that all of the calculations a computer performs are broken down into elementary functions such as addition and multiplication. By this logic, and the application of the chain rule to each of these processes, a computer is able to compute and store a series of partial derivatives, as well as evaluate their values, allowing the derivatives of extremely complex functions to be taken instantaneously. The process is extremely useful in that it automates the process of differentiation through the creation of computational graphs, thus resolving the issue of human error and limitations in manual calculations of derivatives, and is popularly used today in processes such as the creation and training of deep learning models.\n",
    "\n",
    "## Background\n",
    "The derivative is the rate of change of a function with respect to an independent variable. Automatic differentiation evaluates derivatives to machine precision and is less expensive than symbolic differentiation. There are two methods for implementation: forward and reverse. This project implementation focuses on the former of these two processes. Forward-mode automatic differentiation is carried out using the Jacobian $J$, which is a matrix of all of a vector-valued function’s first-order partial derivatives. Instead of doing this symbolically, AD does this by evaluating the chain rule step by step. The chain rule works such that given a function that has two functions nested within each other, the derivative is taken of the outer function first, then multiplied by the derivative of the inner function. For example, say we have the function $f(x)=(x+1)^2$. The rational power of $x$ is an elementary function which we would differentiate first as if we were differentiating $x^2$, after which we would then differentiate the inside elementary function of $x+1$, followed by multiplying the two together. Some other elementary functions include simple addition and multiplication, exponential functions $e^x$, logarithms, trigonometric functions like sine and cosine, and more. Automatic differentiation evaluates these through a graph-like methodology: for a function with many elementary functions and operations, AD will calculate this in order of operations, evaluating at each intermediate step along the way until it arrives at the final result, working much like nodes in a graph. At each step, the derivative is found applying the chain rule along the way. All of these steps are pieced together to work in a forward direction through a function in order of operations.\n",
    "\n",
    "## How to Use\n",
    "### Installation\n",
    "To install this package, run this command on your terminal window (not yet available in this milestone):\n",
    ">pip install [--upgrade] autodiff\n",
    "\n",
    "This should install the required dependencies, such as numpy (and coverage and pytest, which are optional for testing). But you may also manually install numpy using:\n",
    ">pip install numpy\n",
    "\n",
    "Finally, import in the src/autodiff directory in order to use the package:\n",
    ">from autodiff_team44.variable import *\n",
    "\n",
    ">from autodiff_team44.ad import *\n",
    "\n",
    ">from autodiff_team44.functions import *\n",
    "\n",
    "### Basic Demo (Excluding Additional Features)\n",
    "1. Example with 1 scalar input:\n",
    "        x = Variable(-5)\n",
    "        y = (x ** 3 - 4 * x ** 2 + 6 * x - 24) / (-x)\n",
    "y is now a Variable object that contains two attributes, val and der.\n",
    "        print(y)\n",
    "This prints the val and der attributes of y:\n",
    "        f = -55.8\n",
    "        f' = 13.04\n",
    "The evaluation and derivative can be accessed directly using:\n",
    "        y.evaluate()\n",
    "        y.derivative() or y.gradient()\n",
    "The functions derivative() and gradient() are practically interchangeable because we have implemented them to be adaptive depending on whether the function is multi- or single-variate. The Jacobian here would be a scalar, so it would be equivalent to using y.derivative(), but to get the Jacobian:\n",
    "        jacobian(y)\n",
    "\n",
    "\n",
    "2. Example using subscripts to input multiple variables:\n",
    "        x = Variable(np.array([-5, 3.5]))\n",
    "        z1 = (x[0] ** 3 * x[1] ** 2 - 4 * x[0] ** 2 + 6 * x[0] - 24) / (-x[0] * x[1])\n",
    "Again, z1 here is now a Variable object as well that contains val and der attributes, except now those attributes are NumPy arrays since we have multiple variables here. To get the derivative (which would be a gradient in this case of multiple variables):\n",
    "        z1.derivative()\n",
    "or\n",
    "        z1.gradient()\n",
    "or\n",
    "        jacobian(z1)\n",
    "This would return a numpy array representing the gradient. <br>\n",
    "The correct result would be:\n",
    "        [35.8686, -22.4857]\n",
    "The evaluation can also be accessed using:\n",
    "        z1.evaluate()\n",
    "which would return a scalar number\n",
    "\n",
    "\n",
    "3. Example with multiple variables saved separately rather than using subscripting:\n",
    "        x = Variable(-5, np.array([1, 0]))\n",
    "        y = Variable(3.5, np.array([0, 1]))\n",
    "It is important here that the second parameter form a diagonal matrix with the other variables used. \n",
    "        z1 = (x ** 3 * y ** 2 - 4 * x ** 2 + 6 * x - 24) / (-x * y)\n",
    "Like in 1, using print() would print both the val and der attributes:\n",
    "        print(z1)\n",
    "The output would be as follows:\n",
    "        f = -96.3\n",
    "        f' = [35.8686, -22.4857]\n",
    "Again, the gradient as a numpy array can be accessed directly using:\n",
    "        z1.gradient()\n",
    "or\n",
    "        z1.derivative()\n",
    "or\n",
    "        jacobian(z1)\n",
    "The evaluation can also be accessed directly using:\n",
    "        z1.evaluate()\n",
    "which would return a scalar number\n",
    "\n",
    "\n",
    "4. Example that shows how to use the Jacobian function for multiple functions of multiple variables <br>\n",
    "Create a Variable object with two variables taking on the values of -5 and 3.5:\n",
    "        x = Variable(np.array([-5, 3.5]))\n",
    "First function:\n",
    "        z1 = (x[0] ** 3 * x[1] ** 2 - 4 * x[0] ** 2 + 6 * x[0] - 24) / (-x[0] * x[1])\n",
    "Second function:\n",
    "        z2 = x[0] ** 4 + 2.5 * x[1]\n",
    "Third function:\n",
    "        z3 = x[1] ** x[0]\n",
    "Put these functions together in a numpy array:\n",
    "        fns = np.array([z1, z2, z3])\n",
    "The Jacobian as a numpy array (matrix) can be returned using:\n",
    "        jacobian(fns)\n",
    "Correct result:\n",
    "        [35.8686,  -22.4857 ]\n",
    "        [-500,      2.5     ]\n",
    "        [0.002385, -0.002720]\n",
    "\n",
    "\n",
    "5. The same examples from above can be done with callable functions like the following:\n",
    "        def z1(x):\n",
    "            return (x[0] ** 3 * x[1] ** 2 - 4 * x[0] ** 2 + 6 * x[0] - 24) / (-x[0] * x[1])\n",
    "            \n",
    "        x = Variable(np.array([-5, 3.5]))\n",
    "        z1(x).derivative()\n",
    "or\n",
    "        z1(x).gradient()\n",
    "or\n",
    "        jacobian(z1(x))\n",
    "or\n",
    "        jacobian(z1, x)\n",
    "This will yield the same results as above when we did this directly:\n",
    "        z1 = (x[0] ** 3 * x[1] ** 2 - 4 * x[0] ** 2 + 6 * x[0] - 24) / (-x[0] * x[1])\n",
    "\n",
    "\n",
    "**6. CORE DEMO: For callable functions, in fact, Variable objects do not need to be created at all when using the jacobian function to evaluate a scalar derivative, a gradient, or a Jacobian. We can just input function(s) and a scalar value or NumPy array of values at which to evaluate the derivative/gradient/Jacobian:**\n",
    "\n",
    "        def z1(x):\n",
    "            return (x[0] ** 3 * x[1] ** 2 - 4 * x[0] ** 2 + 6 * x[0] - 24) / (-x[0] * x[1])\n",
    "\n",
    "        def z2(x):\n",
    "            return x[0] ** 4 + 2.5 * x[1]\n",
    "\n",
    "        def z3(x):\n",
    "            return x[1] ** x[0]\n",
    "        \n",
    "        fns = np.array([z1, z2, z3])\n",
    "        \n",
    "        jacobian(fns, np.array([-5, 3.5]))\n",
    "**This returns the same results as in example 4 and has the same usage as above in example 5 except that a Variable object is not required to be created in advance of using jacobian():**\n",
    "\n",
    "        [35.8686,  -22.4857 ]\n",
    "        [-500,      2.5     ]\n",
    "        [0.002385, -0.002720]\n",
    "\n",
    "\n",
    "7. Functions such as trigonometric (sin, cos, tan), exponential (exp), inverse trigonometric (arcsin, arccos, arctan), hyperbolic (sin, cosh, tanh), logistic (logis), logarithmic (log), and the square root (sqrt) may also be used in these functions like so:\n",
    "        x = Variable(np.array([np.pi, np.pi/2]))\n",
    "        y = cos(x[0]) + sin(x[1]) - arctan(x[0])\n",
    "        y.derivative()\n",
    "or\n",
    "        y.gradient()\n",
    "or\n",
    "        jacobian(y)\n",
    "We can also use multiple functions:\n",
    "        y2 = exp(x[0]) - log(x[1])\n",
    "        y3 = log(x[0]) * sqrt(x[1])\n",
    "        fns = np.array([y, y2, y3])\n",
    "        jacobian(fns)\n",
    "\n",
    "\n",
    "### Basic Demo of Additional Features\n",
    "1. Higher order derivatives of single variable inputs:\n",
    "        x = Variable(2).higher_order(2)\n",
    "        y = x ** 2\n",
    "        y.derivative()\n",
    "This would return the second derivative of $x^2$ which would be 2.\n",
    "\n",
    "2. Optimization of single variable functions to find local minima and maxima (extrema) using gradient descent and gradient ascent:\n",
    "        def z1(x):\n",
    "            return 2*x**3 + 3*x**2 - 12*x + 5\n",
    "        gd(z1)\n",
    "This would return a local minimum, which is x=1 (caveat: it returns the local minimum closest to the initial value, which by default is 0).\n",
    "        ga(z1)\n",
    "This would return a local maximum, which is x=-2 (caveat: it returns the local maximum closest to the initial value, which by default is 0). We can also use a \"guess\" for where the local minimum or maximum is to find the exact extrema if there are multiple local extrema. The gd and ga functions take the following parameters with the following default values:\n",
    "        gd(fns, initial=0, learn_rate=0.01, max_iters=10000, tol=0.000001, num_vars=1)\n",
    "        ga(fns, initial=0, learn_rate=0.01, max_iters=10000, tol=0.000001, num_vars=1)\n",
    "\n",
    "3. Optimization of multiple variable functions to find local minima and maxima using gradient descent and gradient ascent:\n",
    "        def z1(x):\n",
    "            return 2*x[0]**3 + 3*x[0]**2 - 12*x[0] + 3*x[1]**3 + 4*x[1]**2 - 6*x[1]\n",
    "        gd(z1, num_vars=2)\n",
    "This would return a local minimum, which is x=[1, 0.4852] (caveat: it returns the local minimum closest to the initial value, which by default is [0, 0]).\n",
    "        ga(z1, num_vars=2)\n",
    "This would return a local maximum, which is x=[-2, -1.3741] (caveat: it returns the local maximum closest to the initial value, which by default is [0, 0]). We can also use a \"guess\" for where the local minimum or maximum is to find the exact extrema if there are multiple local extrema. The gd and ga functions take the following parameters with the following default values:\n",
    "        gd(fns, initial=0, learn_rate=0.01, max_iters=10000, tol=0.000001, num_vars=1)\n",
    "        ga(fns, initial=0, learn_rate=0.01, max_iters=10000, tol=0.000001, num_vars=1)\n",
    "\n",
    "4. Optimization of multiple functions of multiple variables to find local minima and maxima using gradient descent and gradient ascent:\n",
    "        def z1(x):\n",
    "            return 2*x[0]**3 + 3*x[0]**2 - 12*x[0] + 3*x[1]**3 + 4*x[1]**2 - 6*x[1]\n",
    "        def z2(x):\n",
    "            return 3*x[0]**3 + 4*x[0]**2 - 13*x[0] + 4*x[1]**3 + 5*x[1]**2 - 7*x[1]\n",
    "        gd(np.array([z1, z2]), num_vars=2)\n",
    "This would return a local minimum, which is x=[[1, 0.48517781][0.8369514, 0.45335888]] (caveat: it returns the local minimum closest to the initial value, which by default is [0, 0]).\n",
    "        ga(np.array([z1, z2]), num_vars=2)\n",
    "This would return a local maximum, which is x=[[-2, 1.3740667 ][-1.72584029, -1.28669221]] (caveat: it returns the local maximum closest to the initial value, which by default is [0, 0]). We can also use a \"guess\" for where the local minimum or maximum is to find the exact extrema if there are multiple local extrema. The gd and ga functions take the following parameters with the following default values:\n",
    "        gd(fns, initial=0, learn_rate=0.01, max_iters=10000, tol=0.000001, num_vars=1)\n",
    "        ga(fns, initial=0, learn_rate=0.01, max_iters=10000, tol=0.000001, num_vars=1)\n",
    "\n",
    "5. Reverse-Mode Auto Differentiation (AD) for a single function with a single variable:\n",
    "\n",
    "        x = ReverseVariable(5)\n",
    "        def y(x):\n",
    "                return exp(x)\n",
    "        \n",
    "        a = y(x)\n",
    "\n",
    "        rev_jacobian(a)\n",
    "\n",
    "        This returns [148.4131591025766], and is the derivative of y with respect to x.\n",
    "\n",
    "6. Reverse-Mode Auto Differentiation (AD) for a single function with multiple variables:\n",
    "\n",
    "        a = ReverseVariable(54)\n",
    "        b = ReverseVariable(2)\n",
    "\n",
    "        def g(a,b):\n",
    "                return a + b - sqrt(a) + exp(b)\n",
    "        \n",
    "        c = g(a, b)\n",
    "        \n",
    "        rev_jacobian(c)\n",
    "\n",
    "        This returns [1.9998285322359397, 61.38905609893065], and represents the partial derivatives of g with respect to both a and b.\n",
    "\n",
    "7. Reverse-Mode Auto Differentiation (AD) for multple functions using a single variable:\n",
    "\n",
    "                x = ReverseVariable(230.3)\n",
    "\n",
    "                def a(x):\n",
    "                        return sqrt(x)\n",
    "\n",
    "                def b(x):\n",
    "                        return sin(x)\n",
    "                \n",
    "                func_a = a(x)\n",
    "                func_b = b(x)\n",
    "\n",
    "                fns = [func_a, func_b]\n",
    "                \n",
    "                rev_jacobian(fns)\n",
    "\n",
    "        This returns: \n",
    "               \n",
    "              [[-5.70455254e-01]\n",
    "               [ 9.42718714e-06]] \n",
    "\n",
    "        and represents the partial derivatives of a with respect to x as well as of b with respect to x.\n",
    "\n",
    "8. Reverse-Mode Auto Differentiation (AD) for multple functions using multiple variables:\n",
    "\n",
    "                x = ReverseVariable(5)\n",
    "                y = ReverseVariable(2)\n",
    "\n",
    "                def a(x,y):\n",
    "                        return sqrt(a) * exp(b)\n",
    "\n",
    "                def b(x,y):\n",
    "                        return sin(a) + b ** b\n",
    "                \n",
    "                func_a = a(x, y)\n",
    "                func_b = b(x, y)\n",
    "\n",
    "                fns = [func_a, func_b]\n",
    "                \n",
    "                rev_jacobian(fns)\n",
    "\n",
    "\n",
    "        This returns the following, which represents the jacobian containing the partial derivatives of a with respect to x and y, as well as the partial derivatives of b with respect to x and y:\n",
    "\n",
    "                [[ 0.14778112 16.52243173]\n",
    "                [ 0.28366219  6.77258872]]\n",
    "\n",
    "        \n",
    "\n",
    "9. These work analogously with more complex functions and with other elementary functions mentioned previously such as trig, exponential, log, square root, etc.\n",
    "\n",
    "## Software Organization\n",
    "On the root directory of our software package, we have three directories for the three portions of our package: the source code for our implementation of automatic differentiation, the required test files that can be run with the software, and the documentation (including a quick start manual). The root directory also contains the LICENSE and README documents. The whole package can be installed through pip as this package is published on PyPI.\n",
    "\n",
    "Within the source code directory, there are three different modules with each including different classes and functions required for the automatic differentiation process.\n",
    "\n",
    "variable.py - This includes implementation of the dual numbers class, named Variable, which includes evaluation, derivative, gradient, and converting the variable to higher order differentiation.\n",
    "\n",
    "ad.py - This includes the implementation of the Jacobian function, which returns the Jacobian given function/input, and the optimization functions using gradient descent and ascent, called gd and ga, which return local minima and maxima, respectively.\n",
    "\n",
    "functions.py - This file includes implementation of the following functions and accounts for input of dual numbers: trigonometric (sin, cos, tan), exponential (exp), inverse trigonometric (arcsin, arccos, arctan), hyperbolic (sin, cosh, tanh), logistic (logis), logarithmic (log), and the square root (sqrt).\n",
    "\n",
    "Within the test directory, we have our test suite of different documents that can be run with our software to test our implementation of automatic differentiation using pytest. This includes three test files, one for each of the files in the source code directory, with the test files being: test_autodiff.py, test_variable.py, and test_functions.py. We also run coverage tests to ensure our code coverage is greater than 90%.\n",
    "\n",
    "Within the documentation directory the milestone 1 markdown document, milestone1.md, milestone 2b markdown document, milestone2_progress.md, the milestone 2 documentation, milestone2.ipynb, and the final documentation document, documentation.ipynb, can be found.\n",
    "\n",
    "Below, you can find how our directory structure will look like with regards to what was explained above:\n",
    "\n",
    "  * autodiff/\n",
    "    * src/\n",
    "      * autodiff_team44/\n",
    "        - \\_\\_init\\_\\_.py\n",
    "        - variable.py #includes implementation of the dual numbers class, Variable, with functions for returning the evaluation, derivative, gradient, and to convert to higher order\n",
    "        - ad.py #includes function that returns the Jacobian given function/input and contains the gd and ga functions for optimization\n",
    "        - functions.py #includes elementary functions when taking in the dual numbers class, Variable, including trig and many more\n",
    "        - rev_variable.py #includes implementation of ReverseVariable class for Reverse-mode AD and many elementary functions which support operations amongst ReverseVariables \n",
    "        - rev_ad.py #includes function to calculate all partial derivatives and another function to return Jacobian given function/input (supports multiple function inputs)\n",
    "    - tests/\n",
    "      - test_autodiff.py #tests the automatic differentiation implementation\n",
    "      - test_variable.py #tests implementation of the dual numbers class, Variable\n",
    "      - test_functions.py #tests implementation of the functions taking in the Variable class\n",
    "    - docs/\n",
    "      - milestone1.md\n",
    "      - milestone2_progress.md\n",
    "      - milestone2.ipynb\n",
    "      - documentation.ipynb\n",
    "    - LICENSE\n",
    "    - README.md\n",
    "    - pyproject.toml\n",
    "\n",
    "Within the src directory, we have the autodiff_team44 directory, which is the same name as our package name published on PyPI. This follows standard convention for directory structure for packages published onto PyPI (https://packaging.python.org/en/latest/tutorials/packaging-projects/).\n",
    "\n",
    "## Minimum Package Requirements\n",
    "### Implementation Details\n",
    "Thus far, we have implemented simple forward mode automatic differentiation using dual numbers in variable.py. The current implementation uses operator overloading for addition, multiplication, subtraction, division, exponentiation (powers), and negation. We also implement functions for exponentials ($e$), trigonometric, inverse trigonometric, hyperbolic, logistic, logarithmic, and square root functions. Through the Variable class, we are able to both evaluate functions, the result of which can be accessed through the .val attribute, as well as first order derivatives, the result of which can be accessed through the .derivative() and .gradient() methods of the Variable class (in addition to the .der attribute, though this is unformatted and thus using the methods are recommended). In the \\_\\_init\\_\\_ function of the Variable class, we store the real (.val) and dual (.der) values that are passed through at initialization of the class as instance attributes, where both val and der can be int, float, or NumPy arrays where der is initialized to 1 by default - assuming it is a variable that the derivative of is to be taken and not a constant - when it is a scalar, and to a matrix with 1's across the diagonal and 0's elsewhere when it is a NumPy array (vector of inputs). We have made our initialization function dynamic such that we can accept int, float, or NumPy arrays and can also accept no value for the dual component. We take advantage of NumPy's built-in features for much easier mathematical operations on NumPy arrays that allow multivariate implementation to be much simpler. We use operator overloading to evaluate the new real and dual components after the corresponding operation, where the evaluation will differ depending on if it is between two dual numbers or between a dual number and an int/float. Likewise, we overload the reverse operations as well (e.g., \\_\\_radd\\_\\_). We acknowledge that such reverse operations will only be called when the operation is between an int or float and a Variable object, so we overloaded these reverse operation functions accordingly by ignoring the case of an operation between two Variable objects, which by definition would not occur. For non-communicative operations such as subtraction, division, and exponentiation, we implement the reverse operations differently than the non-reverse operations. We used algebra, the definition of dual numbers where $\\epsilon^2 = 0$, and dual number concepts to solve for the correct operations to perform on the corresponding real and dual components in each overloaded function. Finally, we also overload the \\_\\_str\\_\\_ and \\_\\_repr\\_\\_ functions so that we can get a nice, easy output of the dual numbers and therefore the result at the end. Most importantly, we overload the \\_\\_getitem\\_\\_ function to make the Variable object subscriptable such that we can handle multiple inputs or variables. We can then do the following to initialize a Variable object with 3 variables:\n",
    "\n",
    "            x = Variable(np.array([a, b, c])), where a, b, and c represent real scalar values\n",
    "\n",
    "We can then get each individual element using x\\[0\\], x\\[1\\], x\\[2\\]. Our implementation allows us to also analogously do:\n",
    "\n",
    "            x = Variable(a, np.array([1, 0, 0]))\n",
    "            y = Variable(b, np.array([0, 1, 0]))\n",
    "            z = Variable(c, np.array([0, 0, 1]))\n",
    "\n",
    "and instead use x, y, and z as the variables. We can use this Variable class to successfully apply forward mode automatic differentiation on different functions, evaluating both the function itself and the derivative of each variable; partial derivatives can be computed using the direction vector that in our implementation is represented in the diagonal matrix of ones for the dual component.\n",
    "\n",
    "We have also implemented in ad.py a function to compute the Jacobian using the dual number implementation above. The function jacobian(fns) checks to make sure that the parameters are valid (a list of NumPy array of Variable objects) and then calls the Variable object .derivative() function that returns the gradient (or a scalar derivative), which is then stacked into a Jacobian. We have built in support for scalar functions of scalar inputs, scalar functions of vector inputs, vector functions of scalar inputs, and vector functions of vector inputs. In other words, the implementation allows for functions from any real dimension to any real dimension. The functions will do the appropriate conditional checks to return the appropriate value depending on the input. The jacobian function also takes on a second parameter, which is x, (jacobian(fns, x)) which is only meant to be used when the function fns is a callable function or NumPy array of callable functions for which the user wants the jacobian at x (the second parameter). The implementation does the appropriate checks to ensure that if fns is callable, x must be included, and otherwise, x does not matter. Additionally, if fns is callable or is a NumPy array of callable functions, the user may input numeric values int and float for x, and the implementation will check for the type and create a Variable object for the user to evaluate at that numeric value (or set of values for multiple variables). \n",
    "\n",
    "**Things to note:**\n",
    "- For the Variable class, it becomes slightly difficult to attempt to compute powers of a real number to a dual number, or a dual number to a dual number, due to the introduction of imaginary and complex numbers, and the need to evaluate complex logarithms. Because of this, we raise an exception if the user attempts to use the Variable class to evaluate a derivative that would result in complex numbers since this project's implementation is limited to real numbers, given the fact that the project prompt does not mention the need for implementation for complex numbers.\n",
    "- Through the Variable class, the implementation does allow the user to directly use dual numbers to compute a scalar derivative for a scalar input or a gradient for multiple inputs such as the following, $f(x,y) = \\frac{x^3y^2 - 4x^2 + 6x - 24}{-xy}$, in addition to using the jacobian() function.\n",
    "- Printing the Variable class returns both the function evaluation and the first derivative or gradient (the implementation automatically returns either a scalar derivative or a NumPy array representing the gradient since the Variable class can handle both int/float and NumPy array). This can be used to print the result (evaluation and derivative) after one or multiple operations.\n",
    "- Although the derivative can be computed using the jacobian() function or by using the Variable class for scalar functions of scalar inputs, we have also implemented the preliminary version of the jacobian() function (as well as .gradient() function), which will return the correct Jacobian for scalar functions of scalar inputs, vector functions of scalar inputs, scalar functions of vector inputs, and vector functions of vector inputs.\n",
    "- The jacobian(fns) function works with just a single function as well as a list or NumPy array of functions.\n",
    "- The function, fns, must contain one or several Variable objects (variables for which to differentiate), or must be a callable function in which case a second parameter x must be inputted that matches the variable(s) in the callable function fns.\n",
    "- The implementation of the Variable class only extends to int, float, and NumPy arrays for the real and dual parts; lists are not supported.\n",
    "\n",
    "**Core data structures:**\n",
    "- Tuples, lists, and NumPy ndarrays\n",
    "\n",
    "**Core classes:**\n",
    "- The Variable class implements dual numbers, which can be used to evaluate functions and their derivatives using forward mode automatic differentiation. The class has two instance attributes, a real part and a dual part, which are stored separately in this class. By virtue of the unique dual aspect, even the most fundamental mathematical operations such as addition and multiplication are performed differently. As such, within the class we redefine methods such as \\_\\_add\\_\\_, \\_\\_sub\\_\\_, \\_\\_mul\\_\\_, \\_\\_pow\\_\\_, etc. such that when a dual number encounters an operation such as “+” or “-”, it evaluates accordingly (via operator overloading). We also implemented derivative() and gradient() functions, which return the appropriate derivative or gradient depending on the number of inputs or variables.\n",
    "\n",
    "**Important attributes:**\n",
    "- val: int, float, or NumPy array of ints and floats; instance attribute in the Variable class; represents the value of the variable(s)\n",
    "- der: int, float, or NumPy array of ints and floats; instance attribute in the Variable; represents the derivative or gradient; optional\n",
    "\n",
    "**External dependencies:**\n",
    "- The NumPy library is needed for evaluating basic functions such as exponentials, trigonometric functions, and logarithms, as well as for its many built-in mathetical features for operations and NumPy's ndarray.\n",
    "\n",
    "**Elementary functions:**\n",
    "- In terms of the elementary functions, within the file functions.py we have implemented the elementary trigonometric, exponential,inverse trigonometric,hyperbolic, logistic, logarithmic, and square root fuctions. For each of these functions, the implementation goes as follows: the function first checks whether the argument inputted is a dual number or not. If the argument, var, is a dual number then, the dual number evaluation of the function is returned. If not, then the function is evaluated using its implementation in the NumPy library, with the exception of the logistic function, which is instead evaluated using the formula for the logistic sigmoid function.\n",
    "\n",
    "**Additional details on implementation:**\n",
    "The Variable class can also be initialized like so:\n",
    "\n",
    "            x = Variable(a, b), where a and b represent a real scalar and the coefficient on ε, respectively, dual defaults to 1 and can be left blank\n",
    "            y = Variable(a, np.array([1, 0])), where a represents a real scalar\n",
    "            z = Variable(b, np.array([0, 1])), where b represents a real scalar\n",
    "\n",
    "From the above, y and z can then be used as multiple variables in a single function, which can then be used to find the gradient of that function. This is analogous to the above where we did x = Variable(np.array(\\[a, b\\])) and used x\\[0\\] and x\\[1\\] instead of x and y. We can then also create a list or NumPy array of multiple functions with these multiple variables and compute the Jacobian from an arbitrary dimension to another arbitrary dimension.\n",
    "\n",
    "Using operator overloading is a key method of implementing automatic differentiation. We utilize operator overloading for many operators and elementary functions such as addition, subtraction, multiplication, division, and exponents. This allows us to take advantage of dual numbers and the Variable class, with the dual part representing the derivative(s) and tangent traces, to calculate both the primal and tangent traces in order from the first operation to the final result, including all intermediate results in the computational graph, and allow for handling operations between both dual and real numbers. With this, order of operations will thus be honored, and the order of steps will be correct. We can do the same with other elementary functions such as sine and natural log. For example, for sine, we can simply have the function check if the argument is a dual number, and if so, it will return the dual number evaluation that contains the derivative as the dual component; otherwise, it will return the regular real evaluation.\n",
    "\n",
    "If we have z = a * (b * c + d) + e, by order of operations, b * c is performed first, then added with d, then multiplied with a, then added with e. With operator overloading, these order of operations are already built-in to fundamental Python operations such as multiplication and addition.\n",
    "\n",
    "## Extension\n",
    "To extend the application of our automatic differentiatior, we have first chosen to tackle the adaptation of the automatic differentiation to higher order derivatives (for single variables), as compared to its previously constrained implementation with only first order derivatives. Higher order derivatives refer to the process of taking derivatives other than, or past, the first derivative, and can be reduced to the definition of derivatives of derivatives. Such a process is useful in application for examples such as illustrating curves and motion problems, and is used to model the travel of transportation, for instance.\n",
    "\n",
    "To implement higher order derivatives, we used recursion. We created a new method within the Variable class called higher_order(order) where this essentially converts a simple Variable object which has val and der attributes to a new Variable object where the val part has a Variable object nested within it. This allows us to utilize operator overloading like with our first order implementation, but it now does so recursively to take the derivative multiple times to get higher order derivatives. Additionally, because the Variable object is now more complex when used with higher order derivatives since the attributes val and der have Variable objects nested within it and can have several layers of this nesting, we updated the derivative() function to return the appropriate derivative by going to appropriate layer to get the requested order's derivative.\n",
    "\n",
    "Additionally, we have implemented optimization to find local extrema (minima and maxima) using gradient descent and ascent.\n",
    "\n",
    "To implement this, we implemented new functions in ad.py called gd(function) where we used Gradient Descent, and ga(function) where we used Gradient Ascent, which are both iterative estimation methods of computing local minima and maxima, respectively. Since they are iterative approximation algorithms, our results here are not perfectly accurate as this is an estimation algorithm, but we set a tolerance threshold and an iteration threshold to try to reach an optimal balance between accuracy and computational expensiveness from all the looping such that the result is accurate enough but doesn't continue to loop for much longer than needed for negligible accuracy differences. This algorithm is very similar to Newton's Method. The caveat to this implementation (and Newton's Method) is that if multiple extrema exist, only one is returned, and the one that is returned is the one that is closest to the initial guess, which by default is 0 unless specified otherwise using the \"initial\" parameter: optimize(function, initial=initial_guess). We have also extended this implementation to work with functions of multiple variables, where these two functions would return a numpy array containing the extrema, and this implementation analogously uses the jacobian or gradient of the function to iterate to arrive at the local minimum or maximum. We then finally also extended this implementation analogously to multiple functions of multiple variables, which uses the above implementation but applies it to multiple functions by looping through each function.\n",
    "\n",
    "Lastly, we have also implemented reverse mode autodifferentiation. With reverse mode AD, it is helpful to visualize a function as a computational graph where each node represents a different variable. To differentiate this from the forward-mode AD, we have implemented a new class called ReverseVariable, which is instantiated with both a value as well as a tuple that contains all the 'child' nodes as well as the local gradients with respect to those children. It may be helpful to illustrate this through a simple example:\n",
    "\n",
    "        x = ReverseVariable(1), which represents 1 as a ReverseVariable\n",
    "        y = ReverseVariable(2), which represents 2 as a ReverseVariable\n",
    "        z = x + y, where z (a ReverseVariable) is the sum of two ReverseVariables\n",
    "        a = x * z, where a (a ReverseVariable) is the product of two ReverseVariables\n",
    "\n",
    "         a                               \n",
    "        / \\ (*)\n",
    "        |   z\n",
    "    (*) |  / \\ (+)\n",
    "        | /   y\n",
    "        |/ (+)\n",
    "        x\n",
    "\n",
    "With this rough visualization, we're able to depict the relationships between these ReverseVariables. For Reverse Mode AD, we represent the edges as local derivatives, which are the derivatives of the 'parent' with respect to the 'child', assuming all other variables are constant. This is how Reverse Mode AD would calculate the derivative of a with respect to x:\n",
    "\n",
    "        da/dx = da/dx_local + da/dz_local * dz/dx_local\n",
    "\n",
    "As you can see, along a path to the desired variable, we multiply all local derivatives for that path. And we sum ACROSS the different paths to calculate the derivative of a ReverseVariable with respect to another. Keeping this all in mind, we've implemented many mathematical functions to account for this such that the 'local_gradients' attribute (which stores the children as well as the respective local derivative) correctly evaluates the local derivative, including addition, subtraction, multiplication, division, inverse, negative, natural log, the basic trigonometric functions, square root and the power function. All of this is done within the rev_variable.py file.\n",
    "\n",
    "Moving onto the rev_ad.py, we have our find_gradients() function which outputs the gradients with respect to the input variables as an array. We have a dictionary (gradients_dict) to track the running partial derivative values with respect to each given ReverseVariable. We then have a helper function child_gradients() which we use to recursively iterate through all nodes of the graph, calculating the local derivatives and updating the running values of the partial derivatives (by multiplying along paths and summing across paths). The val_path_to_child multiplies across a given path, and gradients_dict[child] += val_path_to_child sums the (possibly) multiple paths to a given node. We then kickstart the iteration by calling child_gradients(var, 1); we use 1 as the initial value since the derivative of a variable with respect to itself is simply 1. The way that we sift through and find the leaf nodes (i.e. the input variables) is through the use of sets; we initialize a set near the beginning of the outer function find_gradients() within which we store a ReverseVariable if it has no children (hence, it is an input). Then, at the end, we loop through this set and use each ReverseVariable as a key in the finalized dictionary to retrieve the partial derivative value.\n",
    "\n",
    "Similarly to Forward Mode AD, we've implemented rev_jacobian(), which allows us to take in inputs of multiple functions in the form of lists, NumPy arrays, or tuples and return a Jacobian with the vertically stacked partial derivatives. \n",
    "\n",
    "## Broader Impact and Software Inclusivity\n",
    "At its core, automatic differentiation can be defined as an algorithmic approach to take the derivatives of functions, and thus, is an automation of calculus. More specifically, however, and to differentiate it from other methods of automation of derivative calculation such as finite and symbolic differentiation, automatic differentiation breaks down complex functions into elementary operations and calculates their values, constructing a series of partial derivatives and nodes in a computational graph to retrieve and instantaneously compute first-order derivatives. Practical applications that illustrate the broader impact of automatic differentiation include usage in fields such as: engineering design optimization, computational fluid dynamics. physical modeling, optimal control, structural mechanics, atmospheric sciences, and computational finance. Such widespread applicability illustrates the capacity and power the automatic differentiation provides for complex computations and modeling not afforded by handwritten computations of calculus or other methods of differentiation. Thus, it would allow for increased efficiency, as well as broadened capabilities with respect to complex computations and modeling that would have otherwise not been accessible to society. \n",
    "Of course as we celebrate the positive impacts, it is also important to acknowledge the possible harms or limitations that may be brought about by mass implementation of the software.\n",
    "One such possible harm includes increased energy and resource consumption which could adversely affect the environment and contibute to the existing climate crisis by increasing greenhouse gasses (Okafor, 2020). An additional negative societal impact that the library could have is loss of employment, and consequently income and stability, for individuals who may have previously been needed to compute and/verify calculus solutions by hand. Overally, the impact of utilizing this automatic differentiation library is conceivably net positive. The negative impact on employment is counteracted by the immense quality and quantity of technological advancement that could be realized, and how these could create more employment opportunities for individuals in society. Additionally, its contribution to the ongoing climate crisis is already primarily indirect, and could be mitigated by encouraging users to consider using more renewable sources of energy that are gentler to the environment.\n",
    "\n",
    "We have strived to make our library accessible to different groups. We carefully chose to use the MIT (Expat) License which is very permissive, has very few restrictions, and is very interoparable with many other software licenses, making this library very accessible and non-restrictive for developers who choose to use it.\n",
    "\n",
    "## Future\n",
    "We have made our code open source and and used a permissive license (MIT Expat) to allow users flexibility to implement new features and customize the package to have its implementation best suited to their needs.\n",
    "\n",
    "Additionally, our code is very adaptable. It could be used on varying types of input such as single variables and multivariate functions, as well as varying amounts of input: one function, or many functions, and all output for these could be attained using the same commands.\n",
    "\n",
    "For future extensions to our project, we would like to suggest adding features that expand our current implementation of higher order derivatives to include multivariate functions. \n",
    "\n",
    "Applications:\n",
    "Given the implementation of our optimization using gradient ascent and descent, our code could be used for Machine Learning applications including facial recognition, online fraud detection, medical diagnosis, and automatic language translation.\n",
    "\n",
    "Potential utilizations of the general functioning of automatic differentiation include: engineering design optimization, computational fluid dynamics, physical modeling, structural mechanics, atmospheric sciences, and computational finance.\n",
    "\n",
    "## Works Cited\n",
    "Trvst. “Negative Impact of Technology on the Environment.” TRVST, 8 Dec. 2022, www.trvst.world/environment/negative-impact-of-technology-on-the-environment/. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
